# OMC tutorial

We assume the root path is $OMC, e.g. `/home/chaoliang/MOT`

## Set up environment

`$conda_path` denotes your anaconda path, e.g. `/home/chaoliang/anaconda3`
```
conda create -n OMC python=3.8
source activate OMC
cd OMC/lib/tutorial/OMC/
pip install -r requirements.txt
```


## Testing
### Prepare data and models
1. Download the testing model [[Google Drive]]https://drive.google.com/drive/folders/1lG-bwk22uJjUw5DBy92-h857qMXKJ1Ba?usp=sharing)[[Baidu NetDisk(omct)]](https://pan.baidu.com/s/1mqEFjZJ4Cz00Zy9erl7auw) to `$SOTS/model`.
2. Download testing data e.g. MOT-16 and put them in `$SOTS/dataset`. The dataset can be downloaded from their [official webpage](https://motchallenge.net/).


### Inference
In root path `$SOTS/tracking`,

#### For MOT-16
```
python test_omc.py --weights ../model/OMC_mot17.pt 
				   --cfg ../experiments/model_set/CSTrack_l.yaml 
				   --name l-mot16-test 
				   --test_mot16 True 
				   --output_root runs/test_w_recheck
```

#### For MOT-17
```
python test_omc.py --weights ../model/OMC_mot17.pt 
				   --cfg ../experiments/model_set/CSTrack_l.yaml 
				   --name l-mot17-test 
				   --test_mot17 True 
				   --output_root runs/test_w_recheck
```

#### For MOT-20
```
python test_omc.py --weights ../model/OMC_mot20.pt 
                   --cfg ../experiments/model_set/CSTrack_l.yaml 
                   --name l-mot20-test 
                   --test_mot20 True 
                   --output_root runs/test_w_recheck
```

- Note: If you want to test the performance of the model. Please sign up at [MOT challange](https://motchallenge.net/) and test it.

  

:cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud:
## Training

### Prepare data and models

1. Download the pretrained model which pretrain in COCO dataset [[Google Drive]](https://drive.google.com/drive/folders/181v-yUx15I8dbbemK09qtWojZ2oowgMt?usp=sharing)[[Baidu NetDisk(omct)]](https://pan.baidu.com/s/1GpK_cQhT2KAaJdZcZ3eHNA) to `$SOTS/weights`.

2. We provide several relevant datasets for training and evaluating the CSTrack. 
Annotations are provided in a unified format and all the datasets have the following structure:

```
Caltech
   |——————images
   |        └——————00001.jpg
   |        |—————— ...
   |        └——————0000N.jpg
   └——————labels_with_ids
            └——————00001.txt
            |—————— ...
            └——————0000N.txt
```

Every image has a corresponding annotation text. Given an image path, 
the annotation text path can be generated by replacing the string `images` with `labels_with_ids` and replacing `.jpg` with `.txt`.

In the annotation text, each line is describing a bounding box and has the following format:
```
[class] [identity] [x_center] [y_center] [width] [height]
```
The field `[class]` should be `0`. Only single-class multi-object tracking is supported in this version. 

The field `[identity]` is an integer from `0` to `num_identities - 1`, or `-1` if this box has no identity annotation.

***Note** that the values of `[x_center] [y_center] [width] [height]` are normalized by the width/height of the image, so they are floating point numbers ranging from 0 to 1.


The datasets including Caltech, CityPersons, CUHK-SYSU, PRW, ETHZ and MOT-17 follow [JDE](https://github.com/Zhongdao/Towards-Realtime-MOT). 

#### Caltech Pedestrian
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1sYBXXvQaXZ8TuNwQxMcAgg)
[[1]](https://pan.baidu.com/s/1lVO7YBzagex1xlzqPksaPw) 
[[2]](https://pan.baidu.com/s/1PZXxxy_lrswaqTVg0GuHWg)
[[3]](https://pan.baidu.com/s/1M93NCo_E6naeYPpykmaNgA)
[[4]](https://pan.baidu.com/s/1ZXCdPNXfwbxQ4xCbVu5Dtw)
[[5]](https://pan.baidu.com/s/1kcZkh1tcEiBEJqnDtYuejg)
[[6]](https://pan.baidu.com/s/1sDjhtgdFrzR60KKxSjNb2A)
[[7]](https://pan.baidu.com/s/18Zvp_d33qj1pmutFDUbJyw)

Google Drive: [[annotations]](https://drive.google.com/file/d/1h8vxl_6tgi9QVYoer9XcY9YwNB32TE5k/view?usp=sharing) , 
please download all the images `.tar` files from [this page](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/datasets/USA/) and unzip the images under `Caltech/images`

You may need [this tool](https://github.com/mitmul/caltech-pedestrian-dataset-converter) to convert the original data format to jpeg images.
Original dataset webpage: [CaltechPedestrians](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)
#### CityPersons
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1g24doGOdkKqmbgbJf03vsw)
[[1]](https://pan.baidu.com/s/1mqDF9M5MdD3MGxSfe0ENsA) 
[[2]](https://pan.baidu.com/s/1Qrbh9lQUaEORCIlfI25wdA)
[[3]](https://pan.baidu.com/s/1lw7shaffBgARDuk8mkkHhw)

Google Drive:
[[0]](https://drive.google.com/file/d/1DgLHqEkQUOj63mCrS_0UGFEM9BG8sIZs/view?usp=sharing)
[[1]](https://drive.google.com/file/d/1BH9Xz59UImIGUdYwUR-cnP1g7Ton_LcZ/view?usp=sharing) 
[[2]](https://drive.google.com/file/d/1q_OltirP68YFvRWgYkBHLEFSUayjkKYE/view?usp=sharing)
[[3]](https://drive.google.com/file/d/1VSL0SFoQxPXnIdBamOZJzHrHJ1N2gsTW/view?usp=sharing)

Original dataset webpage: [Citypersons pedestrian detection dataset](https://bitbucket.org/shanshanzhang/citypersons)

#### CUHK-SYSU
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1YFrlyB1WjcQmFW3Vt_sEaQ)

Google Drive:
[[0]](https://drive.google.com/file/d/1D7VL43kIV9uJrdSCYl53j89RE2K-IoQA/view?usp=sharing)

Original dataset webpage: [CUHK-SYSU Person Search Dataset](http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html)

#### PRW
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1iqOVKO57dL53OI1KOmWeGQ)

Google Drive:
[[0]](https://drive.google.com/file/d/116_mIdjgB-WJXGe8RYJDWxlFnc_4sqS8/view?usp=sharing)

Original dataset webpage: [Person Search in the Wild datset](http://www.liangzheng.com.cn/Project/project_prw.html)

#### ETHZ (overlapping videos with MOT-16 removed):
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/14EauGb2nLrcB3GRSlQ4K9Q)

Google Drive:
[[0]](https://drive.google.com/file/d/19QyGOCqn8K_rc9TXJ8UwLSxCx17e0GoY/view?usp=sharing)

Original dataset webpage: [ETHZ pedestrian datset](https://data.vision.ee.ethz.ch/cvl/aess/dataset/)

#### MOT-17
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1lHa6UagcosRBz-_Y308GvQ)

Google Drive:
[[0]](https://drive.google.com/file/d/1ET-6w12yHNo8DKevOVgK1dBlYs739e_3/view?usp=sharing)

Original dataset webpage: [MOT-17](https://motchallenge.net/data/MOT17/)

#### MOT-16 (for evaluation )
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/10pUuB32Hro-h-KUZv8duiw)

Google Drive:
[[0]](https://drive.google.com/file/d/1254q3ruzBzgn4LUejDVsCtT05SIEieQg/view?usp=sharing)

Original dataset webpage: [MOT-16](https://motchallenge.net/data/MOT16/)


#### CrowdHuman
The CrowdHuman dataset can be downloaded from their [official webpage](https://www.crowdhuman.org). The annotation text can be downloaded from the following Baidu NetDisk and Google Drive we provide. 

Baidu NetDisk: 
[[l77e]](https://pan.baidu.com/s/1-wlHeQwizqTN7Ce1tkrXTQ)

Google Drive:
[[0]](https://drive.google.com/file/d/1Q0obzf3WFjFq6bHFjkiHCgRYjU-tUw6d/view?usp=sharing)

Original dataset webpage: [CrowdHuman](https://www.crowdhuman.org)

The CrowdHuman dataset has the following structure:
```
crowdhuman
   |——————images
            |——————train
            |        └——————00001.jpg
            |        |—————— ...
            |        └——————0000N.jpg
            |——————val
            |        └——————00001.jpg
            |        |—————— ...
            |        └——————0000N.jpg
   └——————labels_with_ids
            |——————train
            |        └——————00001.txt
            |        |—————— ...
            |        └——————0000N.txt
            |——————val
            |        └——————00001.txt
            |        |—————— ...
            |        └——————0000N.txt
```

### Start training

1. Modify scripts：Set the dataset path in line2 of `$SOTS/lib/dataset/mot/cfg/*.json`.

2. cd `$OMC/tracking/`


#### for MOT16/MOT17

1). First stage：CSTrack training
```
python train_omc.py --weights ../weights/yolov5l_coco.pt --data ../lib/dataset/mot/cfg/data_ch.json --name l-all --device 0
```

2). Second stage：Train with re-check network
```
python train_omc.py --weights ../runs/train/l-all/weights/best.pt --data ../lib/dataset/mot/cfg/mot17.json --project ../runs/train_w_recheck  --name l-mot17 --device 0 --recheck --noautoanchor
```

#### for MOT20

1). First stage：CSTrack training
```
python train_omc.py --weights ../runs/train/l-all/weights/best.pt  --data ../lib/dataset/mot/cfg/mot20.json --name l-mot20 --device 0
```

2). Second stage：Train with re-check network
```
python train_omc.py --weights ../runs/train/l-mot20/weights/best.pt --data ../lib/dataset/mot/cfg/mot20.json --project ../runs/train_w_recheck  --name l-mot20 --device 0 --recheck --noautoanchor
```

#### Training for own dataset

1). First stage
```
python train_omc.py --weights ../weights/yolov5l_coco.pt or ../model/OMC_mot17.pt
                    --data ../lib/dataset/mot/cfg/xx.json #training on your own dataset
                    --device 0
                    --batch_size 8 
                    --epochs 30     
                    --name project_name                                             
```

2). Second stage
```
python train_omc.py --recheck 
                    --noautoanchor
                    --weights ../runs/train/project_name/weights/best.pt
                    --data ../lib/dataset/mot/cfg/xx.json 
                    --device 0 
```

## References
```
[1] Z. Wang, L. Zheng, et al. Towards real-time multi object tracking. ECCV2020.
[2] C. Liang, Z. Zhang, et al. Rethinking the Competition between Detection and ReID in Multi-Object Tracking. Arxiv2020.
[3] Yolov5. https://github.com/ultralytics/yolov5.
```
